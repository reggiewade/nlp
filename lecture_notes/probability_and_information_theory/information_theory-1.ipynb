{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: before you read this, please go through the probability_theory-discrete and probability_theory-continuous notebooks first. \n",
    "\n",
    "# 1. How informative is my distribution?\n",
    "\n",
    "Wouldn't it be nice to know, given a distribution, how much information it has packed in there?\n",
    "\n",
    "(Some of the material here is taken from McElreath (2015) Chapter 9)\n",
    "\n",
    "## What is Entropy?\n",
    "\n",
    "To illustrate, imagine you are baking a cake. You get out all the ingredients and put them each into their own respective bowls. Then you put everything into a big mixing bowl and you begin to mix. You mix so much that you get to a point where mixing any more would make no difference because the random shuffling of all of the ingredients due to additional mixing would yield an equally random shuffling of ingredients. \n",
    "\n",
    "This is an example of entropy at its two extremes. At one extreme, we have *minimum* entropy where all of the ingredients are distinct and identifiable in their respective bowls. At the other extreme, we have the point where additional mixing would not increase the combination of the ingredients. This is called *maximum* entropy. \n",
    "\n",
    "It turns out that this principle is quite useful in probability theory: probability distributions that are fairly even (i.e., all of the possible combinations of all values of all variables yield roughly similar probabilities) is when we have high entropy. When we have it so only one or two variable values carry most of the probability mass, we have low entropy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, what is the entropy of a coin toss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P(X=''):\n",
    "    if X == 'heads': return 0.5\n",
    "    if X == 'tails': return 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: it's going to be high (whatever a high value of entropy might be). \n",
    "\n",
    "What is the entropy that it will be cold or snowy in Boise in January in 2017?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P(X=''):\n",
    "    if X == 'cold-snowy':     return 0.95\n",
    "    if X == 'not-cold-snowy': return 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entropy here will be low because a lot of the probability mass is held by one variable value. \n",
    "\n",
    "Entropy is calculated as follows:\n",
    "\n",
    "### $$H(p) = - \\sum_{x \\in X} P(X=x) log(P(X=x)) $$\n",
    "\n",
    "Notice! This is how it is computed for discrete as well as continuous distributions. The entropy is based on the *data*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the entropies of the two distributions above. First, make sure you press shift-enter on the first one then come back to this one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6931471805599453"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "- (P('heads') * math.log(P('heads')) + P('tails') * math.log(P('tails')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, press shift-enter on the second probability function then come back to this one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1985152433458726"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "- (P('cold-snowy') * math.log(P('cold-snowy')) + P('not-cold-snowy') * math.log(P('not-cold-snowy')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we suspected, the entropy for the 50/50 case was much higher than for the 95/05 case. \n",
    "\n",
    "Let's look at this a bit closer. What if we have a binary variable, and we make a plot where the y axis is the entropy, and the x axis is the amount of probability mass in one of the variable values? Let's make the variable `X` take on the values `a` and `b`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = {}\n",
    "X['a'] = [i * 0.01 for i in range(1,100)] # list of 0.01 to 0.99\n",
    "X['b'] = [1 - (i * 0.01) for i in range(1,100)] # list of 0.99 to 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.79, 0.20999999999999996)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check to make sure one of the value pairs sum to one:\n",
    "X['a'][78], X['b'][78]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define entropy\n",
    "def H(p):\n",
    "    return p * math.log(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_axis = [-(H(a) + H(b)) for a,b in zip(X['a'], X['b'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f641c06ad30>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline  \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(0,len(y_axis)), y_axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question:\n",
    "\n",
    "![alt text](ytho.jpg \"Title\")\n",
    "\n",
    "Answer:\n",
    "\n",
    "* It begins with variable value `a` with 0.99 and variable value `b` with 0.01, which is very low entropy. Then as mass is moved from `a` to `b`, the two values slowly move closer to each other, until they reach 0.5/0.5 which is the highest entropy that can be had in a distribution over one variable with two values. Then mass continues to be moved from `a` to `b`, until it's `b` that has 0.99 and `a` that has 0.1, which is back to low entropy. \n",
    "* It doesn't matter which variable has the high probability mass; entropy is agnostic to that. It only cares about the actual probabilities themselves. \n",
    "* Note that if we used another log base like `ln` the answer would be different (see reading). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What makes entropy useful?\n",
    "\n",
    "There are many ways to obtain information from a distribution. We've seen some ways already, such as the mean and the standard deviation. Another thing to look at could be the maximum and minimum value. Entropy is yet a way of looking at a distribution, but only the probabilities themselves. \n",
    "\n",
    "#### Entropy turns out to be quite useful, indeed. That is because we can use it to:\n",
    "\n",
    "* Determine how certain a distribution is (i.e., low entropy means more mass is focused around one or two variable values, so with lower entropy we know that something like a classifier that is producing a distribution over possible class labels is more decided on one value than another)\n",
    "* When dealing with finding the right probability density function for a continuous distribution (remember our Gaussian and cat weights), we can use entropy. The best function for an underlying distribution is the function that maximizes the entropy. I.e., the function with the highest entropy is the best fit (why? because it distributes the probability mass more evenly).\n",
    "* We can use entropy to compare the difference between two distributions (this is very, very useful in neural networks and certain kinds of classifiers). We'll discuss this a bit more below.\n",
    "* It's a more informative measure of a distribution than the mean because a mean could be the same for a lot of different distributions (e.g., (1+2+3)/3 = 2, but (2+2+2)/3=2, yet the entropy for the latter distribution is higher).\n",
    "* It's easy to calculate. It fulfills the criteria that (1) it's a continuous measure (i.e., the entropy value is a real number) (2) it increases as possible events increase and (3) it is additive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Principle of Maximum Entropy\n",
    "\n",
    "(p.269) The principle of maxiumum entropy applies to this measure of uncertainty to the problem of choosing among probability distirbutions. Perhaps the simplest way to state the maxiumum entropy principle is:\n",
    "\n",
    "* The distribution that can happen the most ways is also the distribution with the biggest information entropy. The distribution with the biggest information entropy is the most conservative distribution that conveys its constraints. \n",
    "- Note that we're using summation for continuous distributions, but really one should calculate the integral. It turns out that summing works (as long as one uses the set) as an estimate of the entropy, though ideally you'd want to calculate the integral. Because we are treating this discrete-ish, the uniform distribution function is the discrete version. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from scipy.stats import norm  \n",
    "\n",
    "data = pd.read_csv('catsM.csv')\n",
    "\n",
    "col = list(set(data.Bwt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian\n",
    "\n",
    "- needs mean, std\n",
    "\n",
    "\n",
    "##### $n(x; \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{\\frac{-(x - \\mu)^2}{2\\sigma^2}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.429532639676383"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# estimate params\n",
    "mean = np.mean(col)\n",
    "std = np.std(col)\n",
    "H = - np.sum([norm.pdf(x,mean,std) * math.log(norm.pdf(x,mean,std)) for x in col])\n",
    "\n",
    "H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pareto\n",
    "\n",
    "- alpha\n",
    "\n",
    "#### $f_x(x, \\alpha) = \\frac{\\alpha}{x^{\\alpha + 1}}   $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.012026856502975952"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parameter estimation\n",
    "n = len(data)\n",
    "x_hat_m = min(col) \n",
    "def pareto_mle(d):\n",
    "    return n /  np.sum([math.log(x / x_hat_m) for x in d])\n",
    "    \n",
    "alpha = pareto_mle(col)\n",
    "#function definition\n",
    "def pareto(x,a):\n",
    "    return a / (x ** (a + 1))\n",
    "\n",
    "H = - np.sum([pareto(x,alpha) * math.log(pareto(x,alpha)) for x in col])\n",
    "\n",
    "H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exponential\n",
    "\n",
    "- lambda\n",
    "\n",
    "#### $ f(x,\\lambda)  = \\lambda e^{-\\lambda x}  $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.195598703729821"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# estimate lambda param\n",
    "l  = 1.0 / np.mean(col)\n",
    "\n",
    "# define pdf\n",
    "def exponential(x,l):\n",
    "    return l * math.exp(- l * x)\n",
    "\n",
    "H = - np.sum([exponential(x,l) * math.log(exponential(x,l)) for x in col])\n",
    "\n",
    "H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uniform\n",
    "\n",
    "- n\n",
    "\n",
    "#### $ f(x) =  \\frac{1}{n}  $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.995732273553991"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = len(col)\n",
    "\n",
    "def uniform(x,n):\n",
    "    return 1.0 / n\n",
    "\n",
    "H = - np.sum([uniform(x,n) * math.log(uniform(x,n)) for x in col])\n",
    "\n",
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Comparing two distributions\n",
    "\n",
    "Suppose you have a distribution `P` and another distribution `Q` (i.e., you write two functions) both which take the same variable `X`, which, of course, takes on certain values. Both `P` and `Q` sum to one, of course, but they do so in a different manner (i.e., the probability mass is distributed differently over `X`'s values). \n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P(X=''):\n",
    "    if X == 'sunny': return 0.3\n",
    "    if X == 'rainy': return 0.7\n",
    "    \n",
    "def Q(X=''):\n",
    "    if X == 'sunny': return 0.6\n",
    "    if X == 'rainy': return 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we compare the two distributions?\n",
    "\n",
    "The entropy of P is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6108643020548935"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "- (P('sunny') * math.log(P('sunny')) + P('rainy') * math.log(P('rainy')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6730116670092565"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "- (Q('sunny') * math.log(Q('sunny')) + Q('rainy') * math.log(Q('rainy')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we expected that, since Q's distribution is \"flatter\" (i.e., the distribution of mass is more evenly spread across the values), then the entropy is higher. \n",
    "\n",
    "Question:\n",
    "\n",
    "* *But how can we directly compare the two distributions? Wouldn't it be more informative to compare the distrbutions value by value?*\n",
    "\n",
    "Answer:\n",
    "\n",
    "* Absolutely. For that, we can adjust the entropy equation to make the comparison directly (I'll forego the gory mathematical details):\n",
    "\n",
    "### $$ D_{KL}(P||Q) = \\sum_i P(i) log \\big(\\frac{P(i)}{Q(i)}\\big) $$\n",
    "\n",
    "This is called the **Kullback-Leibler Divergence (or KL-Divergence)**.\n",
    "\n",
    "What is the KL-Divergence for our little example above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18378689738681217"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(P('sunny') * math.log(P('sunny') / Q('sunny')) + P('rainy') * math.log(P('rainy') / Q('rainy')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question:\n",
    "\n",
    "* *What does that number mean?*\n",
    "\n",
    "Answer:\n",
    "\n",
    "* At the moment, nothing really. It means something when you start to make changes to the distribution. \n",
    "\n",
    "Let's change things a bit. Let's keep P the same but change Q, then try KL-Divergence again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Q(X=''):\n",
    "    if X == 'sunny': return 0.5\n",
    "    if X == 'rainy': return 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08228287850505178"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(P('sunny') * math.log(P('sunny') / Q('sunny')) + P('rainy') * math.log(P('rainy') / Q('rainy')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KL-Divergence got smaller. Let's go one more step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Q(X=''):\n",
    "    if X == 'sunny': return 0.4\n",
    "    if X == 'rainy': return 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02160085414354654"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(P('sunny') * math.log(P('sunny') / Q('sunny')) + P('rainy') * math.log(P('rainy') / Q('rainy')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's even smaller. \n",
    "\n",
    "Question:\n",
    "\n",
    "* *Why did that happen?*\n",
    "\n",
    "Answer:\n",
    "\n",
    "* Look at how the Q distribution changed. It's probability distirbution over the variable values moved closer to what they are for P. That means the divergence between the two distributions got smaller. \n",
    "\n",
    "Question:\n",
    "\n",
    "* *What happens when the two distirbutions are the same?*\n",
    "\n",
    "Answer:\n",
    "\n",
    "* Let's give it a try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Q(X=''):\n",
    "    if X == 'sunny': return 0.3\n",
    "    if X == 'rainy': return 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(P('sunny') * math.log(P('sunny') / Q('sunny')) + P('rainy') * math.log(P('rainy') / Q('rainy')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, by definition, there is no divergence between them. \n",
    "\n",
    "Note: the divergence is not communiative. I.e. $D_{KL}(P||Q)$ is not the same as $D_{KL}(Q||P)$. In general, the first distribution (in our examples, `P` is the distribution that we want `Q` to be like). \n",
    "\n",
    "Question:\n",
    "\n",
    "* *I can kind of see why entropy is useful, but how is KL Divergence is useful?*\n",
    "\n",
    "Answer:\n",
    "\n",
    "* Think of it this way: it is a measure of how much information could be gained when the probability distribution for Q is revised to the probability distribution P. Put another (perhaps more intuitive way): it's the information *lost* when using Q to approximate P. \n",
    "\n",
    "Question:\n",
    "\n",
    "* *So....why is that useful?*\n",
    "\n",
    "Dr. K's explanation:\n",
    "\n",
    "* Imagine you are trying to write a probability distribution/function for some phenomenon so you can predict it (like the weather or a language phenomenon). You start with a joint probability distribution, then you make independence assuptions and factor your joint probability distribution (using the chain rule) into several smaller, more managable conditional probability distributions. Then you take some data and count up what you need for your probability functions/distributions. But then you want to see if your probabilistic model works the way you want it to; i.e., it produces probabilities that make sense and reflect reality. One way to test that is to take a part of your data and see if your model's predictions (i.e., the distributions that it generates) matches what is expected. \n",
    "* Neural networks use this a lot for training. They produce distributions and that is checked using something like KL Divervence to compare. If the prediction is moving away from the \"gold\" distribution, then something is wrong with the underlying network so the training regime sends a signal back through the neurons/nodes so they can adjust in hopes that the adjustments will yield better distributions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
